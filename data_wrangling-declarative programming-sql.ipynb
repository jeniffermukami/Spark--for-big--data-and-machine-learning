{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb83af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ceac5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x26f112c31c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "SparkSession.builder.config(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c055a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb0094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22951ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData wrangling with Spark SQL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:302\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# If encryption is enabled, we need to setup a server in the jvm to read broadcast\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# data via a socket.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# scala's mangled names w/ $ in them require special treatment.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encryption_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39misEncryptionEnabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc)\n\u001b[0;32m    301\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_AUTH_SOCKET_TIMEOUT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetPythonAuthSocketTimeout\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc)\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_BUFFER_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetSparkBufferSize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc))\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpythonExec \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYSPARK_PYTHON\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\tools\\spark-3.0.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1530\u001b[0m, in \u001b[0;36mJavaClass.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_return_value(\n\u001b[0;32m   1528\u001b[0m             answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m   1531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe4a10",
   "metadata": {},
   "source": [
    "#### Create a View And Run Queries\n",
    "​\n",
    "The code below creates a temporary view against which you can run SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM user_log_table LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57269faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT * \n",
    "          FROM user_log_table \n",
    "          LIMIT 2\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT COUNT(*) \n",
    "          FROM user_log_table \n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT userID, firstname, page, song\n",
    "          FROM user_log_table \n",
    "          WHERE userID == '1046'\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT page\n",
    "          FROM user_log_table \n",
    "          ORDER BY page ASC\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb04440",
   "metadata": {},
   "source": [
    "#### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT *, get_hour(ts) AS hour\n",
    "          FROM user_log_table \n",
    "          LIMIT 1\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d232d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d406f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b422553a",
   "metadata": {},
   "source": [
    "#### Converting Results to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ac7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff956dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(songs_in_hour_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd2eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7029e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
